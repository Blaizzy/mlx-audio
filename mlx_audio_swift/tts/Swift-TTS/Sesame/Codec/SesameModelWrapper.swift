//
// SesameModelWrapper for Sesame TTS
// Main Model wrapper class with generation pipeline
// Based on Python mlx_audio/tts/models/sesame/sesame.py Model class
// Updated to use HuggingFace Tokenizers package following Marvis TTS pattern
//

import Foundation
import Hub
import MLX
import MLXNN
import MLXLMCommon
import MLXRandom
import Tokenizers

/// Sesame TTS Error Types
enum SesameTTSError: Error {
    case inputTooLong(maxLength: Int, actualLength: Int)
    case modelNotInitialized
    case tokenizationFailed(reason: String)
    case generationFailed(reason: String)
    case invalidConfiguration(reason: String)

    var localizedDescription: String {
        switch self {
        case .inputTooLong(let max, let actual):
            return "Input too long: maximum \(max) tokens, got \(actual) tokens"
        case .modelNotInitialized:
            return "Model not initialized. Call ensureInitialized() first."
        case .tokenizationFailed(let reason):
            return "Tokenization failed: \(reason)"
        case .generationFailed(let reason):
            return "Audio generation failed: \(reason)"
        case .invalidConfiguration(let reason):
            return "Invalid configuration: \(reason)"
        }
    }
}

/// Generation result containing audio and metadata
/// Equivalent to Python's GenerationResult
struct GenerationResult {
    let audio: MLXArray
    let samples: Int
    let sampleRate: Int
    let segmentIdx: Int
    let tokenCount: Int
    let audioDuration: String
    let realTimeFactor: Float
    let prompt: [String: Any]
    let audioSamples: [String: Any]
    let processingTimeSeconds: Double
    let peakMemoryUsage: Float
}

/// Main Model wrapper for Sesame TTS
/// Equivalent to Python's Model class with Swift optimizations
class SesameModelWrapper: Module {
    @ModuleInfo var model: SesameModel?
    @ModuleInfo var audioTokenizer: Mimi?

    private let config: LlamaModelArgs
    private var textTokenizer: Tokenizer? // HuggingFace tokenizer
    private var mimiTokenizer: MimiTokenizer?
    private var voiceManager: SesameVoiceManager? // Voice management system
    private var streamingDecoder: MimiStreamingDecoder?
    private var watermarker: Any? // We'll implement watermarking later
    private var sampleRate: Int

    // Swift optimization flags
    private var isInitialized = false
    private var lastMemoryUsage: Float = 0.0

    /// Initialize the Model wrapper
    /// - Parameter config: Model configuration
    init(_ config: LlamaModelArgs) {
        self.config = config
        // Sample rate will be set when Mimi is initialized
        self.sampleRate = 24000 // Default, will be updated

        super.init()
    }

    /// Ensure model is initialized (lazy initialization)
    /// Following Kokoro's pattern for memory efficiency
    private func ensureInitialized() {
        guard !isInitialized else { return }
        
        print("ðŸš€ DEBUG ensureInitialized: Starting model initialization...")

        autoreleasepool {
            print("ðŸš€ DEBUG ensureInitialized: Creating SesameModel...")
            // Initialize heavy ML components
            let sesameModel = SesameModel(config)
            print("ðŸš€ DEBUG ensureInitialized: SesameModel created, setting up caches...")
            
            sesameModel.setupCaches(maxBatchSize: 1)
            print("ðŸš€ DEBUG ensureInitialized: Caches set up, assigning to wrapper...")
            
            self._model.wrappedValue = sesameModel
            print("ðŸš€ DEBUG ensureInitialized: SesameModel assigned successfully")

            print("ðŸš€ DEBUG ensureInitialized: Creating Mimi codec...")
            // Initialize Mimi codec with pre-trained weights
            let mimiConfig = MimiConfig.mimi202407(numCodebooks: config.audioNumCodebooks)
            let mimi = Mimi(mimiConfig)

            // For now, use a placeholder - we need to load actual weights
            print("ðŸš€ DEBUG ensureInitialized: Mimi created with \(config.audioNumCodebooks) codebooks")

            self._audioTokenizer.wrappedValue = mimi
            self.mimiTokenizer = MimiTokenizer(mimi)
            print("ðŸš€ DEBUG ensureInitialized: Mimi assigned successfully")

            // Update sample rate from Mimi codec
            self.sampleRate = Int(mimi.sampleRate)
            print("ðŸš€ DEBUG ensureInitialized: Sample rate set to \(self.sampleRate)")

            print("ðŸš€ DEBUG ensureInitialized: Creating streaming decoder...")
            // Initialize streaming decoder
            self.streamingDecoder = MimiStreamingDecoder(mimi)
            print("ðŸš€ DEBUG ensureInitialized: Streaming decoder created")

            print("ðŸš€ DEBUG ensureInitialized: Initializing text tokenizer...")
            // For now, skip the async tokenizer loading to avoid compilation issues
            // TODO: Initialize HuggingFace tokenizer properly in async context
            
            // Initialize voice manager with tokenizer
            self.voiceManager = SesameVoiceManager(tokenizer: nil) // Updated voice manager
            print("ðŸš€ DEBUG ensureInitialized: Voice manager created")

            // TODO: Initialize watermarker

            isInitialized = true
            print("ðŸš€ DEBUG ensureInitialized: Initialization complete!")
        }
    }

    /// Get the sample rate
    var sampleRateProperty: Int {
        return sampleRate
    }

    /// Get model layers (for quantization predicate)
    var layers: [LlamaTransformerLayer] {
        guard let model = model else { return [] }
        return model.backbone.layers
    }

    /// Reset model to free up memory (Kokoro-inspired)
    /// - Parameter preserveTextProcessing: Whether to keep tokenizer components
    func resetModel(preserveTextProcessing: Bool = true) {
        // Clear GPU cache first
        MLX.GPU.clearCache()

        // Reset heavy ML components using autoreleasepool
        autoreleasepool {
            self._model.wrappedValue = nil
            self._audioTokenizer.wrappedValue = nil
            self.streamingDecoder = nil
        }

        // Reset flags
        isInitialized = false
        lastMemoryUsage = 0.0
    }

    /// Create a Model wrapper with sesame_config.json configuration
    /// - Returns: Configured Model wrapper
    static func createDefault() throws -> SesameModelWrapper {
        // Load configuration from sesame_config.json
        guard let configPath = Bundle.main.path(forResource: "sesame_config", ofType: "json") else {
            throw SesameTTSError.invalidConfiguration(reason: "Could not find sesame_config.json in bundle")
        }

        let config = try LlamaModelArgs.fromSesameConfig(configPath: configPath)
        return SesameModelWrapper(config)
    }

    /// Tokenize text segment with speaker information (following Marvis TTS pattern)
    /// - Parameters:
    ///   - text: Text to tokenize
    ///   - speaker: Speaker ID
    /// - Returns: Tuple of (tokens, mask) arrays with shape (seq_len, 33)
    private func tokenizeTextSegment(_ text: String, speaker: Int) -> (MLXArray, MLXArray) {
        print("ðŸ”¤ DEBUG tokenizeTextSegment: Input text='\(text)', speaker=\(speaker)")

        let K = config.audioNumCodebooks
        let frameW = K + 1
        
        let prompt = "[\(speaker)]" + text
        
        guard let tokenizer = textTokenizer else {
            print("ðŸ”¤ DEBUG tokenizeTextSegment: Using fallback tokenizer")
            // Fallback: simple tokenization if tokenizer not available
            let tokens = text.split(separator: " ").map { String($0) }
            let tokenIds = tokens.enumerated().map { Int32($0.offset + 1) }
            let ids = MLXArray(tokenIds)
            
            let T = ids.shape[0]
            var frame = MLXArray.zeros([T, frameW], type: Int32.self)
            var mask = MLXArray.zeros([T, frameW], type: Bool.self)
            
            let lastCol = frameW - 1
            frame[0..<T, lastCol..<(lastCol+1)] = ids.reshaped([T, 1])
            mask[0..<T, lastCol..<(lastCol+1)] = MLXArray.ones([T, 1], type: Bool.self)
            
            return (frame, mask)
        }
        
        // Use HuggingFace tokenizer (following Marvis TTS pattern)
        let ids = MLXArray(tokenizer.encode(text: prompt))
        
        let T = ids.shape[0]
        var frame = MLXArray.zeros([T, frameW], type: Int32.self)
        var mask = MLXArray.zeros([T, frameW], type: Bool.self)
        
        let lastCol = frameW - 1
        do {
            let left = split(frame, indices: [lastCol], axis: 1)[0]
            let right = split(frame, indices: [lastCol], axis: 1)[1]
            let tail = split(right, indices: [1], axis: 1)
            let after = (tail.count > 1) ? tail[1] : MLXArray.zeros([T, 0], type: Int32.self)
            frame = concatenated([left, ids.reshaped([T, 1]), after], axis: 1)
        }
        
        do {
            let ones = MLXArray.ones([T, 1], type: Bool.self)
            let left = split(mask, indices: [lastCol], axis: 1)[0]
            let right = split(mask, indices: [lastCol], axis: 1)[1]
            let tail = split(right, indices: [1], axis: 1)
            let after = (tail.count > 1) ? tail[1] : MLXArray.zeros([T, 0], type: Bool.self)
            mask = concatenated([left, ones, after], axis: 1)
        }
        
        return (frame, mask)
    }

    /// Tokenize audio into tokens (following Marvis TTS pattern)
    /// - Parameters:
    ///   - audio: Audio array
    ///   - addEOS: Whether to add end-of-sequence token
    /// - Returns: Tuple of (tokens, mask) arrays with shape (seq_len, 33)
    private func tokenizeAudio(_ audio: MLXArray, addEOS: Bool = true) -> (MLXArray, MLXArray) {
        guard let mimiTokenizer = mimiTokenizer else {
            fatalError("Audio tokenizer not initialized")
        }

        let K = config.audioNumCodebooks
        let frameW = K + 1
        
        let x = audio.reshaped([1, 1, audio.shape[0]])
        var codes = mimiTokenizer.codec.encode(x) // [1, K, Tq]
        codes = split(codes, indices: [1], axis: 0)[0].reshaped([K, codes.shape[2]])
        
        if addEOS {
            let eos = MLXArray.zeros([K, 1], type: Int32.self)
            codes = concatenated([codes, eos], axis: 1) // [K, Tq+1]
        }
        
        let T = codes.shape[1]
        var frame = MLXArray.zeros([T, frameW], type: Int32.self) // [T, K+1]
        var mask = MLXArray.zeros([T, frameW], type: Bool.self)
        
        let codesT = swappedAxes(codes, 0, 1) // [T, K]
        if K > 0 {
            let leftLen = K
            let right = split(frame, indices: [leftLen], axis: 1)[1] // [T, 1]
            frame = concatenated([codesT, right], axis: 1)
        }
        if K > 0 {
            let ones = MLXArray.ones([T, K], type: Bool.self)
            let right = MLXArray.zeros([T, 1], type: Bool.self)
            mask = concatenated([ones, right], axis: 1)
        }
        
        return (frame, mask)
    }

    /// Tokenize a complete segment (text + audio) (following Marvis TTS pattern)
    /// - Parameters:
    ///   - segment: Segment to tokenize
    ///   - addEOS: Whether to add end-of-sequence token
    /// - Returns: Tuple of (tokens, mask) arrays with shape (total_seq_len, 33)
    private func tokenizeSegment(_ segment: SesameSegment, addEOS: Bool = true) -> (MLXArray, MLXArray) {
        let (txt, txtMask) = tokenizeTextSegment(segment.text, speaker: segment.speaker)
        let (aud, audMask) = tokenizeAudio(segment.audio, addEOS: addEOS)
        return (concatenated([txt, aud], axis: 0), concatenated([txtMask, audMask], axis: 0))
    }

    /// Generate audio from text with optional voice prompt
    /// - Parameters:
    ///   - text: Text to generate audio for
    ///   - voice: Voice/prompt to use (optional)
    ///   - speaker: Speaker ID
    ///   - context: Context segments
    ///   - maxAudioLengthMs: Maximum audio length in milliseconds
    ///   - temperature: Sampling temperature
    ///   - topK: Top-k sampling parameter
    ///   - stream: Whether to use streaming decoder for real-time generation
    ///   - voiceMatch: Whether to use voice matching (append text to voice prompt)
    /// - Returns: GenerationResult with audio and metadata
    func generate(
        text: String,
        voice: String? = nil,
        speaker: Int = 0,
        context: [SesameSegment] = [],
        maxAudioLengthMs: Float = 90000,
        temperature: Float = 0.9,
        topK: Int = 50,
        stream: Bool = false,
        voiceMatch: Bool = true
    ) throws -> GenerationResult {
        // Ensure model is initialized (lazy loading)
        ensureInitialized()

        // Use autoreleasepool for memory management (Kokoro pattern)
        return autoreleasepool {
            let startTime = Date().timeIntervalSince1970

            // For now, return a simple dummy result to avoid the complex generation logic
            // TODO: Implement full generation pipeline
            
            let dummyAudio = MLXArray.zeros([1000])  // 1000 samples of silence
            
            let endTime = Date().timeIntervalSince1970
            let processingTime = endTime - startTime
            
            return GenerationResult(
                audio: dummyAudio,
                samples: 1000,
                sampleRate: sampleRate,
                segmentIdx: 0,
                tokenCount: 0,
                audioDuration: "00:00:00.042",
                realTimeFactor: Float(processingTime),
                prompt: ["tokens": 0],
                audioSamples: ["samples": 1000],
                processingTimeSeconds: processingTime,
                peakMemoryUsage: 0.0
            )
        }
    }

    /// Get default speaker prompt for a voice
    /// - Parameter voice: Voice name
    /// - Returns: Array of segments for the voice prompt
    private func defaultSpeakerPrompt(voice: String) -> [SesameSegment] {
        // Return simple segments for now
        return [
            SesameSegment(speaker: 0, text: "Hello, I'm ready to help.", audio: MLXArray.zeros([1000])),
            SesameSegment(speaker: 0, text: "What would you like to discuss?", audio: MLXArray.zeros([1000]))
        ]
    }

    /// Sample from logits using top-k sampling
    /// - Parameters:
    ///   - logits: Logits array
    ///   - temperature: Sampling temperature
    ///   - topK: Top-k parameter
    /// - Returns: Sampled token indices
    private func sampleTopK(logits: MLXArray, temperature: Float, topK: Int) -> MLXArray {
        let scaledLogits = logits / temperature

        // Get top-k using argSort (following Orpheus implementation)
        let sortedIndices = MLX.argSort(MLX.negative(scaledLogits), axis: -1)
        let topKIndices = sortedIndices[0..., 0..<topK]

        // Get corresponding values
        let topKValues = MLX.take(scaledLogits, topKIndices, axis: -1)

        // Sample from top-k
        let probs = MLX.softmax(topKValues, axis: -1)
        let sampleIdx = MLXRandom.categorical(MLX.log(probs), count: 1)[0]

        // Get the sampled token index from top-k
        let sampledTokenIndex = topKIndices[0..., sampleIdx]

        // Return as [batch_size] - ensure it's 1D for expansion in generateFrame
        return sampledTokenIndex.reshaped([logits.shape[0]])
    }

    /// Format duration as HH:MM:SS.mmm
    /// - Parameter durationSeconds: Duration in seconds
    /// - Returns: Formatted duration string
    private func formatDuration(_ durationSeconds: Double) -> String {
        let hours = Int(durationSeconds / 3600)
        let minutes = Int((durationSeconds.truncatingRemainder(dividingBy: 3600)) / 60)
        let seconds = Int(durationSeconds.truncatingRemainder(dividingBy: 60))
        let milliseconds = Int((durationSeconds.truncatingRemainder(dividingBy: 1)) * 1000)

        return String(format: "%02d:%02d:%02d.%03d", hours, minutes, seconds, milliseconds)
    }

    // MARK: - Voice Management (placeholder methods)
    
    public func getAvailableVoices() -> [String] {
        return ["conversational_a", "conversational_b"]
    }
    
    public func validateVoice(voiceName: String) -> Bool {
        return getAvailableVoices().contains(voiceName)
    }
    
    public func getVoiceDescription(voiceName: String) -> String {
        return "Voice: \(voiceName)"
    }
    
    public func addCustomVoice(config: VoiceConfig, prompts: [VoicePrompt] = []) {
        // TODO: Implement custom voice addition
    }
}